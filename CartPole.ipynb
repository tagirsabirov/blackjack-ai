{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import gym\n",
    "import sys\n",
    "import h5py\n",
    "from tensorflow import keras\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "state = env.reset()\n",
    "env.seed(100)\n",
    "\n",
    "states = env.observation_space.shape[0]\n",
    "actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.02756547 -0.04249852  0.04271414  0.04248675]\n",
      "Discrete(2)\n",
      "Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print(state)\n",
    "print(env.action_space)\n",
    "print(env.observation_space)\n",
    "print(env.observation_space.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score26.0\n",
      "Episode:2 Score19.0\n",
      "Episode:3 Score13.0\n",
      "Episode:4 Score29.0\n",
      "Episode:5 Score14.0\n",
      "Episode:6 Score18.0\n",
      "Episode:7 Score15.0\n",
      "Episode:8 Score15.0\n",
      "Episode:9 Score22.0\n",
      "Episode:10 Score22.0\n"
     ]
    }
   ],
   "source": [
    "episodes = 10\n",
    "for i in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    while not done:\n",
    "        env.render()\n",
    "        action=random.choice([0,1])\n",
    "        n_state, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "    print('Episode:{} Score{}'.format(i, score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_9 (Dense)              (None, 1, 24)             120       \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1, 12)             300       \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1, 2)              26        \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 446\n",
      "Trainable params: 446\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.Input(shape=(1, states)))\n",
    "model.add(keras.layers.Dense(24, activation='relu'))\n",
    "model.add(keras.layers.Dense(12, activation='relu'))\n",
    "model.add(keras.layers.Dense(actions, activation='linear'))\n",
    "model.add(keras.layers.Flatten())\n",
    "print(model.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/keras/optimizer_v2/optimizer_v2.py:355: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 50000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "    1/10000 [..............................] - ETA: 18:28 - reward: 1.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 50s 5ms/step - reward: 1.0000\n",
      "57 episodes - episode_reward: 173.526 [121.000, 200.000] - loss: 1.503 - mae: 24.174 - mean_q: 48.404\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 1.0000\n",
      "57 episodes - episode_reward: 176.509 [10.000, 200.000] - loss: 1.021 - mae: 28.593 - mean_q: 57.111\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 50s 5ms/step - reward: 1.0000\n",
      "52 episodes - episode_reward: 191.385 [150.000, 200.000] - loss: 2.229 - mae: 30.256 - mean_q: 60.513\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 50s 5ms/step - reward: 1.0000\n",
      "51 episodes - episode_reward: 196.647 [151.000, 200.000] - loss: 4.320 - mae: 32.734 - mean_q: 65.444\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 1.0000\n",
      "done, took 256.962 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd72a54bd30>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy = EpsGreedyQPolicy()\n",
    "memory = SequentialMemory(limit=50000, window_length=1)\n",
    "dqn = DQNAgent(model=model, memory=memory ,policy=policy, nb_steps_warmup=10, \n",
    "                nb_actions=actions, target_model_update=1e-2)\n",
    "\n",
    "\n",
    "dqn.compile(keras.optimizers.Adam(lr=1e-3), metrics=['mae'])\n",
    "\n",
    "dqn.fit(env, nb_steps=50000, visualize=False, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 185.000, steps: 185\n",
      "Episode 2: reward: 179.000, steps: 179\n",
      "Episode 3: reward: 192.000, steps: 192\n",
      "Episode 4: reward: 189.000, steps: 189\n",
      "Episode 5: reward: 179.000, steps: 179\n",
      "Episode 6: reward: 200.000, steps: 200\n",
      "Episode 7: reward: 189.000, steps: 189\n",
      "Episode 8: reward: 196.000, steps: 196\n",
      "Episode 9: reward: 199.000, steps: 199\n",
      "Episode 10: reward: 185.000, steps: 185\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd72a54b8b0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.test(env, nb_episodes=10, visualize=True)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
